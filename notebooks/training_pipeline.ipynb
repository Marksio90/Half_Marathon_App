{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e561c05",
   "metadata": {},
   "source": [
    "# ðŸƒ Half Marathon Time Prediction - Training Pipeline\n",
    "## Professional ML Pipeline with Digital Ocean Spaces Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install boto3 pandas numpy scikit-learn xgboost joblib python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a8201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import joblib\n",
    "import os\n",
    "from io import BytesIO, StringIO\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b528b0",
   "metadata": {},
   "source": [
    "## ðŸ” Configuration & Digital Ocean Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55205f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to Digital Ocean Spaces: half-marathon-app\n"
     ]
    }
   ],
   "source": [
    "# Digital Ocean Spaces Configuration\n",
    "DO_SPACES_KEY = os.getenv('DO_SPACES_KEY', 'your_access_key')\n",
    "DO_SPACES_SECRET = os.getenv('DO_SPACES_SECRET', 'your_secret_key')\n",
    "DO_SPACES_REGION = os.getenv('DO_SPACES_REGION', 'fra1')\n",
    "DO_SPACES_BUCKET = os.getenv('DO_SPACES_BUCKET', 'halfmarathon-ml')\n",
    "DO_SPACES_ENDPOINT = f'https://{DO_SPACES_REGION}.digitaloceanspaces.com'\n",
    "\n",
    "# Initialize S3 client (Digital Ocean Spaces is S3-compatible)\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    region_name=DO_SPACES_REGION,\n",
    "    endpoint_url=DO_SPACES_ENDPOINT,\n",
    "    aws_access_key_id=DO_SPACES_KEY,\n",
    "    aws_secret_access_key=DO_SPACES_SECRET\n",
    ")\n",
    "\n",
    "print(f\"âœ… Connected to Digital Ocean Spaces: {DO_SPACES_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad570fab",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Data Loading from Digital Ocean Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_from_spaces(filename):\n",
    "    \"\"\"Load CSV file from Digital Ocean Spaces\"\"\"\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=DO_SPACES_BUCKET, Key=filename)  # â† BEZ data/\n",
    "        df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')), sep=';')\n",
    "        print(f\"âœ… Loaded {filename}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load datasets\n",
    "df_2023 = load_csv_from_spaces('halfmarathon_wroclaw_2023__final.csv')\n",
    "df_2024 = load_csv_from_spaces('halfmarathon_wroclaw_2024__final.csv')\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([df_2023, df_2024], ignore_index=True)\n",
    "print(f\"\\nðŸ“Š Total combined dataset: {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70080a1",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_seconds(time_str):\n",
    "    \"\"\"Convert time format HH:MM:SS to seconds\"\"\"\n",
    "    if pd.isnull(time_str) or time_str in ['DNS', 'DNF', '']:\n",
    "        return None\n",
    "    try:\n",
    "        parts = str(time_str).split(':')\n",
    "        if len(parts) == 3:\n",
    "            return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_age(birth_year):\n",
    "    \"\"\"Calculate age from birth year\"\"\"\n",
    "    current_year = datetime.now().year\n",
    "    try:\n",
    "        return current_year - int(birth_year)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(\"ðŸ”„ Starting data cleaning...\")\n",
    "\n",
    "# Convert time columns to seconds\n",
    "time_columns = ['5 km Czas', '10 km Czas', '15 km Czas', '20 km Czas', 'Czas']\n",
    "for col in time_columns:\n",
    "    if col in df.columns:\n",
    "        df[f'{col}_seconds'] = df[col].apply(convert_time_to_seconds)\n",
    "\n",
    "# Calculate age\n",
    "if 'Rocznik' in df.columns:\n",
    "    df['Wiek'] = df['Rocznik'].apply(calculate_age)\n",
    "\n",
    "# Encode gender\n",
    "if 'PÅ‚eÄ‡' in df.columns:\n",
    "    le_gender = LabelEncoder()\n",
    "    df['PÅ‚eÄ‡_encoded'] = le_gender.fit_transform(df['PÅ‚eÄ‡'].fillna('M'))\n",
    "    joblib.dump(le_gender, 'gender_encoder.pkl')\n",
    "\n",
    "# Remove rows with missing target variable\n",
    "df_clean = df[df['Czas_seconds'].notna()].copy()\n",
    "\n",
    "# Remove outliers (times > 4 hours or < 1 hour)\n",
    "df_clean = df_clean[\n",
    "    (df_clean['Czas_seconds'] >= 3600) & \n",
    "    (df_clean['Czas_seconds'] <= 14400)\n",
    "]\n",
    "\n",
    "print(f\"âœ… Data cleaning complete: {len(df_clean)} valid rows\")\n",
    "print(f\"ðŸ“‰ Removed {len(df) - len(df_clean)} invalid/outlier rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f5ae0",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Feature Selection & Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_cols = [\n",
    "    'PÅ‚eÄ‡_encoded',\n",
    "    'Wiek',\n",
    "    '5 km Czas_seconds',\n",
    "    '5 km Tempo',\n",
    "]\n",
    "\n",
    "# Add optional features if available\n",
    "optional_features = ['10 km Tempo', '15 km Tempo', 'Tempo StabilnoÅ›Ä‡']\n",
    "for feat in optional_features:\n",
    "    if feat in df_clean.columns:\n",
    "        feature_cols.append(feat)\n",
    "\n",
    "target_col = 'Czas_seconds'\n",
    "\n",
    "# Create feature matrix\n",
    "df_model = df_clean[feature_cols + [target_col]].dropna()\n",
    "\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[target_col]\n",
    "\n",
    "print(f\"ðŸ“Š Feature matrix shape: {X.shape}\")\n",
    "print(f\"ðŸ“‹ Features used: {feature_cols}\")\n",
    "print(f\"\\nðŸ“ˆ Target variable statistics:\")\n",
    "print(f\"   Mean: {y.mean()/60:.2f} minutes\")\n",
    "print(f\"   Median: {y.median()/60:.2f} minutes\")\n",
    "print(f\"   Std: {y.std()/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0f227",
   "metadata": {},
   "source": [
    "## ðŸ”€ Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a32d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"âœ… Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6f43c",
   "metadata": {},
   "source": [
    "## ðŸ¤– Model Training & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ad9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Training XGBoost model with hyperparameter tuning...\\n\")\n",
    "\n",
    "# Define parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize XGBoost\n",
    "xgb_model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nâœ… Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"âœ… Best CV MAE: {-grid_search.best_score_/60:.2f} minutes\")\n",
    "\n",
    "# Use best model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f63d0",
   "metadata": {},
   "source": [
    "## ðŸ“Š Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {dataset_name} Metrics:\")\n",
    "    print(f\"   MAE: {mae/60:.2f} minutes ({mae:.0f} seconds)\")\n",
    "    print(f\"   RMSE: {rmse/60:.2f} minutes ({rmse:.0f} seconds)\")\n",
    "    print(f\"   RÂ² Score: {r2:.4f}\")\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2}\n",
    "\n",
    "train_metrics = evaluate_model(y_train, y_pred_train, \"Training Set\")\n",
    "test_metrics = evaluate_model(y_test, y_pred_test, \"Test Set\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nðŸŽ¯ Feature Importance:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332d58d",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save Model Locally & Upload to Digital Ocean Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e626303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model metadata\n",
    "model_metadata = {\n",
    "    'model_type': 'XGBoost',\n",
    "    'version': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    'features': feature_cols,\n",
    "    'train_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'metrics': {\n",
    "        'train': train_metrics,\n",
    "        'test': test_metrics\n",
    "    },\n",
    "    'best_params': grid_search.best_params_\n",
    "}\n",
    "\n",
    "# Save model locally\n",
    "model_filename = f\"halfmarathon_model_{model_metadata['version']}.pkl\"\n",
    "metadata_filename = f\"model_metadata_{model_metadata['version']}.pkl\"\n",
    "\n",
    "joblib.dump(best_model, model_filename)\n",
    "joblib.dump(model_metadata, metadata_filename)\n",
    "\n",
    "print(f\"âœ… Model saved locally: {model_filename}\")\n",
    "print(f\"âœ… Metadata saved locally: {metadata_filename}\")\n",
    "\n",
    "# Upload to Digital Ocean Spaces\n",
    "def upload_to_spaces(local_file, spaces_key):\n",
    "    try:\n",
    "        s3_client.upload_file(\n",
    "            local_file,\n",
    "            DO_SPACES_BUCKET,\n",
    "            f'models/{spaces_key}',\n",
    "            ExtraArgs={'ACL': 'private'}\n",
    "        )\n",
    "        print(f\"âœ… Uploaded to Spaces: models/{spaces_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Upload error: {e}\")\n",
    "\n",
    "upload_to_spaces(model_filename, model_filename)\n",
    "upload_to_spaces(metadata_filename, metadata_filename)\n",
    "upload_to_spaces('gender_encoder.pkl', 'gender_encoder.pkl')\n",
    "\n",
    "# Also save as \"latest\" version\n",
    "upload_to_spaces(model_filename, 'halfmarathon_model_latest.pkl')\n",
    "upload_to_spaces(metadata_filename, 'model_metadata_latest.pkl')\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f249dcd",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862baa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample data\n",
    "test_cases = [\n",
    "    {'PÅ‚eÄ‡_encoded': 1, 'Wiek': 30, '5 km Czas_seconds': 1200, '5 km Tempo': 4.0},\n",
    "    {'PÅ‚eÄ‡_encoded': 0, 'Wiek': 25, '5 km Czas_seconds': 1500, '5 km Tempo': 5.0},\n",
    "    {'PÅ‚eÄ‡_encoded': 1, 'Wiek': 45, '5 km Czas_seconds': 1350, '5 km Tempo': 4.5},\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Test Predictions:\\n\")\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    # Create feature vector with proper column names\n",
    "    test_df = pd.DataFrame([case])\n",
    "    # Add missing columns with default values\n",
    "    for col in feature_cols:\n",
    "        if col not in test_df.columns:\n",
    "            test_df[col] = 0\n",
    "    test_df = test_df[feature_cols]\n",
    "    \n",
    "    prediction_seconds = best_model.predict(test_df)[0]\n",
    "    prediction_time = f\"{int(prediction_seconds//3600)}:{int((prediction_seconds%3600)//60):02d}:{int(prediction_seconds%60):02d}\"\n",
    "    \n",
    "    gender = 'MÄ™Å¼czyzna' if case['PÅ‚eÄ‡_encoded'] == 1 else 'Kobieta'\n",
    "    print(f\"Test case {i}:\")\n",
    "    print(f\"   {gender}, {case['Wiek']} lat, 5km: {case['5 km Czas_seconds']//60}:{case['5 km Czas_seconds']%60:02d}\")\n",
    "    print(f\"   Predicted half-marathon time: {prediction_time}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "halfmarathon_pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
